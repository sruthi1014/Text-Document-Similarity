{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk -----commenting after the first run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sruth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sruth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the files\n",
    "text1=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\uiuc.txt\",\"rt\", encoding=\"utf8\")\n",
    "uiuc=text1.read()\n",
    "text1.close()\n",
    "\n",
    "text2=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\uis.txt\",\"rt\", encoding=\"utf8\")\n",
    "uis=text2.read()\n",
    "text2.close()\n",
    "\n",
    "text3=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\mit.txt\",\"rt\", encoding=\"utf8\")\n",
    "mit=text3.read()\n",
    "text3.close()\n",
    "\n",
    "text4=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\stanford.txt\",\"rt\", encoding=\"utf8\")\n",
    "stanford=text4.read()\n",
    "text4.close()\n",
    "\n",
    "text5=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\tesla.txt\",\"rt\", encoding=\"utf8\")\n",
    "tesla=text5.read()\n",
    "text5.close()\n",
    "\n",
    "text6=open(r\"E:\\MS_Studies\\sem2\\566\\hw1\\uic.txt\",\"rt\", encoding=\"utf8\")\n",
    "uic=text6.read()\n",
    "text6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#Data cleaning\n",
    "#-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step a: function to remove punctuation/apostrophe/convert to lower\n",
    "def removal(textfile):\n",
    "    y=textfile\n",
    "    ##step 1: converting to lower case\n",
    "    y=y.lower()\n",
    "    \n",
    "    #step 2: replacing apostraphe\n",
    "    y=y.replace(\"'\", \" \")\n",
    "    \n",
    "    # step 3: remove_punctuation\n",
    "    y=y.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step b: call to function removal\n",
    "uic=removal(uic)\n",
    "uiuc=removal(uiuc)\n",
    "uis=removal(uis)\n",
    "mit=removal(mit)\n",
    "stanford=removal(stanford)\n",
    "tesla=removal(tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for performing tokenization/filtering/stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    x=text\n",
    "    #step4: tokenization ( split using word tokenizer)\n",
    "    #tk = word_tokenize()  \n",
    "    x=word_tokenize(x)\n",
    "\n",
    "    # step5: filtering out tokens that are not alphabetic\n",
    "    x= [c for c in x if c.isalpha()]\n",
    "    \n",
    "    #step6: removing stop words\n",
    "    list_stop_words = set(stopwords.words('english'))\n",
    "    x      = [w for w in x if not w in list_stop_words]\n",
    "    \n",
    "    # step7: Stemming\n",
    "    ps = PorterStemmer()\n",
    "    x= [ps.stem(c) for c in x]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uic=process(uic)\n",
    "uiuc=process(uiuc)\n",
    "uis=process(uis)\n",
    "mit=process(mit)\n",
    "stanford=process(stanford)\n",
    "tesla=process(tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "#       MEASURING SIMILARITIES\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#Jaccard Similarity\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find jaccard similarity\n",
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1) \n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    print(c)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uiuc_jac = get_jaccard_sim(uic,uiuc)\n",
    "uis_jac = get_jaccard_sim(uic,uis)\n",
    "mit_jac = get_jaccard_sim(uic,mit)\n",
    "stanford_jac = get_jaccard_sim(uic,stanford)\n",
    "tesla_jac = get_jaccard_sim(uic,tesla)\n",
    "\n",
    "list_jac=[uiuc_jac,uis_jac,mit_jac,stanford_jac,tesla_jac]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28652668416447946, 0.24639878695981804, 0.2163361941402782, 0.2668453292496171, 0.19344262295081968]\n"
     ]
    }
   ],
   "source": [
    "print(list_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "### Cosine similarity\n",
    "#-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenizing the documents\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "uic1=TreebankWordDetokenizer().detokenize(uic)\n",
    "uiuc1=TreebankWordDetokenizer().detokenize(uiuc)\n",
    "uis1=TreebankWordDetokenizer().detokenize(uis)\n",
    "mit1=TreebankWordDetokenizer().detokenize(mit)\n",
    "stanford1=TreebankWordDetokenizer().detokenize(stanford)\n",
    "tesla1=TreebankWordDetokenizer().detokenize(tesla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing and finding cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "documents=[uic1,uiuc1,uis1,mit1,stanford1,tesla1]\n",
    "tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "##finding similarity of UIC with others using cosine_similarity function available in the SKlearn \n",
    "similarity=cosine_similarity(tfidf[0:1],tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x6074 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11107 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.66742406, 0.46350624, 0.24144094, 0.2822682 ,\n",
       "        0.09593293]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity[0:1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.66742406, 0.46350624, 0.24144094, 0.2822682 ,\n",
       "        0.09593293]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
